<div class="user">
  <div class="project">
    <h2>Hidden Markov Model</h2>
      <img alt="First Image" src="{{ "/assets/img/hmm_generator.png" | prepend: site.baseurl }}" width = "350" height "70" />
      <img alt="First Image" src="{{ "/assets/img/generation_example.png" | prepend: site.baseurl }}" width = "350" height "70" />
      <p>A Hidden Markov Model (HMM) is a machine learning process that are commonly used in Natural Language Processing (NLP)<b>.</b> Modern neutral network methods that most companies used for their NLP rely on or mimic HMM, as they are computationally efficient and extremely adept at unsupervised learning for NLP<b>.</b> This HMM, built from only Pandas, was originally built to do word suggestion<b>.</b> This was then implemented for sentence generation<b>.</b> The data for training was from a text base of all of Shakespeare’s plays (an easily found source)<b>.</b> Dictionaries were used, as using a matrix of all the unique number of words, N, would be NxN in size<b>.</b> This matrix would mostly be sparse, potentially unable to be rendered in memory<b>.</b> Dictionaries were much faster due to their hash-table implementation<b>.</b> We then calculate all the probabilities needed and create a sample distribution from which to sample words<b>.</b> If you would like to look more into the project, here is the GitHub <a href = "https://github.com/AustinJAdams/EECS-738/blob/master/AutoCorrectandTextGeneration/Notebooks/AutocorrectAndGenerationusingShakespeare.ipynb">repo<b>.</b></a></p>
    </div>
</div>
<div class="user">
    <div class="project">
      <h2>Hidden Markov Model</h2>
        <img alt="First Image" src="{{ "/assets/img/hmm_generator.png" | prepend: site.baseurl }}" width = "350" height "70" />
        <img alt="First Image" src="{{ "/assets/img/generation_example.png" | prepend: site.baseurl }}" width = "350" height "70" />
        <p>For my Machine Learning Class, we were given were four projects, and this was one<b>.</b> We were required to create a Hidden Markov Model using only Pandas<b>.</b> Using this Hidden Markov Model (HMM), we were to use some text (text locus) to do word suggestion and sentence generation<b>.</b> If you have never heard of a Hidden Markov Model or its usage, here is an informative <a href = "https://towardsdatascience.com/introduction-to-hidden-markov-models-cd2c93e6b781">article</a> on the subject<b>.</b></p>
        <p>Our plan was to create probability dictionaries for each word<b>.</b> The desire would be getting all the probabilities that a third word would come after the first two<b>.</b> We could have added another word, making the previous three words forecast the fourth, but this proved too small a sample size to get a good variation<b>.</b> For a set of two words, to choose the best third, we created a dictionary of all the probabilities of any third word coming two words after the first word in the string and another dictionary for one after the second word in the string<b>.</b> An example would be “I am ____", attempting to forecast the ___<b>.</b> To do this, we would find the probability that a word comes two after I and the probability that a word comes one after am<b>.</b> When the function was asked to generate a third word, it would input the previous two words into the two dictionaries, multiply the resulting answers (creating a “joint” dictionary), and take the word with the highest probability<b>.</b> In this example “___” was “the<b>.</b>” <p>
        <p>As a quick design aside, using a dictionary was better than an array, as even though NumPy arrays are faster, the array was sparse, meaning most entries were 0, thus dictionaries were faster, as they did not input the zeroes<b>.</b> This was our first attempt, but it ran into the problem that there was no variability<b>.</b> Instead, we used this joint probability dictionary to mimic a probability distribution and when you call the function, it samples from the created sample distribution, thus giving variability to the third word while still giving informed answers<b>.</b></p>
        <p>The code for this project is in our GitHub repo - EECS 738 but can also be found <a href = "https://github.com/AustinJAdams/EECS-738/blob/master/AutoCorrectandTextGeneration/Notebooks/AutocorrectAndGenerationusingShakespeare.ipynb">here</a></p> 
    </div>
  </div>